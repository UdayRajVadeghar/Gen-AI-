# Tokenization :

## What is tokenization:

Tokenization is the process of breaking text into smaller pieces (tokens) so that AI models can understand and process it.

**Example**:
ðŸ“œ Text: "Hello, world!"
ðŸ”¹ Tokens: ["Hello", ",", " world", "!"]

AI doesn't read words like humansâ€”it converts them into tokens and numbers to analyze and generate responses.
