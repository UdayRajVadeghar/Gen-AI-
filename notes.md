# Tokenization :

## What is tokenization:

Tokenization is the process of breaking text into smaller pieces (tokens) so that AI models can understand and process it.

**Example**:
Text: "Hello, world!" <br>
Tokens: ["Hello", ",", " world", "!"]<br>
Tokens: [9906, 11, 1917, 0]<br>
length: 4 tokens<br>

AI doesn't read words like humansâ€”it converts them into tokens and numbers to analyze and generate responses.
