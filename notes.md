# Tokenization :

## What is tokenization:

Tokenization is the process of breaking text into smaller pieces (tokens) so that AI models can understand and process it.

**Example**:
Text: "Hello, world!"
Tokens: ["Hello", ",", " world", "!"]
Tokens: [9906, 11, 1917, 0]
length: 4 tokens

AI doesn't read words like humansâ€”it converts them into tokens and numbers to analyze and generate responses.
